{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1915925ba58>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return x / 255\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    temp = np.zeros([len(x), 10])\n",
    "    temp[range(len(x)), x] = 1        \n",
    "\n",
    "    return temp\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    "If you're finding it hard to dedicate enough time for this course a week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) to build each layer, except \"Convolutional & Max Pooling\" layer.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    "If you would like to get the most of this course, try to solve all the problems without TF Layers.  Let's begin!\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a bach of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    shp = (None, ) + image_shape\n",
    "    x = tf.placeholder(tf.float32, shape=shp, name='x')\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape=(None, n_classes), name='y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "Note: You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for this layer.  You're free to use any TensorFlow package for all the other layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    #print(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    depth = int(x_tensor.shape[3])\n",
    "    weight = tf.Variable(tf.truncated_normal([conv_ksize[0], conv_ksize[1], depth, conv_num_outputs]))\n",
    "    bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    \n",
    "    conv_layer = tf.nn.conv2d(x_tensor, weight, strides=[1, conv_strides[0], conv_strides[1], 1], padding='SAME')\n",
    "    conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    \n",
    "    pool_layer = tf.nn.max_pool(conv_layer, [1, pool_ksize[0], pool_ksize[1], 1], [1, pool_strides[0], pool_strides[1], 1], \n",
    "                                'SAME')\n",
    "    return pool_layer \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). You can use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    size = int(x_tensor.shape[1]) * int(x_tensor.shape[2]) * int(x_tensor.shape[3])\n",
    "    rv = tf.reshape(x_tensor, [-1, size])\n",
    "    \n",
    "    return rv\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). You can use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    shape = int(x_tensor.shape[1])\n",
    "    weight = tf.Variable(tf.truncated_normal([shape, num_outputs], stddev=0.03))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    dense = tf.matmul(x_tensor, weight)\n",
    "    dense = tf.add(dense, bias)\n",
    "    dense = tf.nn.relu(dense)\n",
    "    \n",
    "    return dense\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). You can use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for this layer.\n",
    "\n",
    "Note: Activation, softmax, or cross entropy shouldn't be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    shape = int(x_tensor.shape[1])\n",
    "    weight = tf.Variable(tf.truncated_normal([shape, num_outputs], stddev=0.03))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    dense = tf.matmul(x_tensor, weight)\n",
    "    dense = tf.add(dense, bias)\n",
    "    \n",
    "    return dense\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "    depth = 32\n",
    "    conv_ksize = 2\n",
    "    conv_stride = 1\n",
    "    pool_ksize = 4\n",
    "    net = conv2d_maxpool(x, depth, (conv_ksize, conv_ksize), (conv_stride, conv_stride), \n",
    "                         (pool_ksize, pool_ksize), (pool_ksize, pool_ksize))\n",
    "    \n",
    "    depth = 64\n",
    "    conv_ksize = 2\n",
    "    conv_stride = 1\n",
    "    pool_ksize = 2\n",
    "    net = conv2d_maxpool(x, depth, (conv_ksize, conv_ksize), (conv_stride, conv_stride), \n",
    "                         (pool_ksize, pool_ksize), (pool_ksize, pool_ksize))\n",
    "    \n",
    "    depth = 128\n",
    "    conv_ksize = 2\n",
    "    conv_stride = 1\n",
    "    pool_ksize = 2\n",
    "    net = conv2d_maxpool(x, depth, (conv_ksize, conv_ksize), (conv_stride, conv_stride), \n",
    "                         (pool_ksize, pool_ksize), (pool_ksize, pool_ksize))\n",
    "\n",
    "    \n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    net = flatten(net)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    \n",
    "    net = fully_conn(net, 1024)\n",
    "    net = tf.nn.dropout(net, keep_prob)\n",
    "    net = fully_conn(net, 256)\n",
    "    \n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    \n",
    "    net = output(net, 10)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return net\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer, feed_dict={\n",
    "                x: feature_batch,\n",
    "                y: label_batch,\n",
    "                keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    loss = session.run(cost, feed_dict={\n",
    "                x: feature_batch,\n",
    "                y: label_batch,\n",
    "                keep_prob: 1.})\n",
    "    valid_acc = session.run(accuracy, feed_dict={\n",
    "                x: valid_features,\n",
    "                y: valid_labels,\n",
    "                keep_prob: 1.})\n",
    "    print('Loss: {:.6f} Val_Acc: {:.6f}'.format(\n",
    "                loss,\n",
    "                valid_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 100\n",
    "batch_size = 512\n",
    "keep_probability = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss: 2.260054 Val_Acc: 0.144000\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss: 2.105421 Val_Acc: 0.228000\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss: 1.963520 Val_Acc: 0.252000\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss: 1.878829 Val_Acc: 0.300000\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss: 1.788319 Val_Acc: 0.364000\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss: 1.660107 Val_Acc: 0.424000\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss: 1.531639 Val_Acc: 0.432000\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss: 1.407771 Val_Acc: 0.458000\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss: 1.304692 Val_Acc: 0.462000\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss: 1.184302 Val_Acc: 0.488000\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss: 1.085262 Val_Acc: 0.496000\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss: 0.966516 Val_Acc: 0.514000\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss: 0.892942 Val_Acc: 0.512000\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss: 0.802532 Val_Acc: 0.530000\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss: 0.707439 Val_Acc: 0.544000\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss: 0.686600 Val_Acc: 0.538000\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss: 0.608059 Val_Acc: 0.538000\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss: 0.575935 Val_Acc: 0.546000\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss: 0.530909 Val_Acc: 0.552000\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss: 0.524065 Val_Acc: 0.542000\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss: 0.453022 Val_Acc: 0.554000\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss: 0.428575 Val_Acc: 0.528000\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss: 0.388931 Val_Acc: 0.554000\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss: 0.364332 Val_Acc: 0.548000\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss: 0.339609 Val_Acc: 0.548000\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss: 0.301935 Val_Acc: 0.562000\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss: 0.282771 Val_Acc: 0.548000\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss: 0.286927 Val_Acc: 0.540000\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss: 0.288649 Val_Acc: 0.536000\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss: 0.254405 Val_Acc: 0.540000\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss: 0.282751 Val_Acc: 0.530000\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss: 0.245977 Val_Acc: 0.556000\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss: 0.238137 Val_Acc: 0.548000\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss: 0.282107 Val_Acc: 0.532000\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss: 0.240639 Val_Acc: 0.548000\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss: 0.211795 Val_Acc: 0.548000\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss: 0.187803 Val_Acc: 0.556000\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss: 0.144375 Val_Acc: 0.570000\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss: 0.143614 Val_Acc: 0.568000\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss: 0.148766 Val_Acc: 0.554000\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss: 0.136053 Val_Acc: 0.542000\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss: 0.135949 Val_Acc: 0.548000\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss: 0.115016 Val_Acc: 0.572000\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss: 0.116715 Val_Acc: 0.554000\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss: 0.137876 Val_Acc: 0.528000\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss: 0.100473 Val_Acc: 0.540000\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss: 0.096327 Val_Acc: 0.556000\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss: 0.101835 Val_Acc: 0.548000\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss: 0.118208 Val_Acc: 0.552000\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss: 0.072742 Val_Acc: 0.550000\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss: 0.069193 Val_Acc: 0.570000\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss: 0.068425 Val_Acc: 0.546000\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss: 0.072176 Val_Acc: 0.554000\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss: 0.048610 Val_Acc: 0.558000\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss: 0.047218 Val_Acc: 0.566000\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss: 0.048255 Val_Acc: 0.564000\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss: 0.043488 Val_Acc: 0.590000\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss: 0.047505 Val_Acc: 0.574000\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss: 0.030384 Val_Acc: 0.584000\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss: 0.036922 Val_Acc: 0.556000\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss: 0.018867 Val_Acc: 0.580000\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss: 0.016425 Val_Acc: 0.586000\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss: 0.023812 Val_Acc: 0.570000\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss: 0.031244 Val_Acc: 0.576000\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss: 0.029742 Val_Acc: 0.588000\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss: 0.028355 Val_Acc: 0.586000\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss: 0.020401 Val_Acc: 0.572000\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss: 0.021814 Val_Acc: 0.584000\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss: 0.030554 Val_Acc: 0.592000\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss: 0.011781 Val_Acc: 0.604000\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss: 0.018570 Val_Acc: 0.576000\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss: 0.015127 Val_Acc: 0.578000\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss: 0.005055 Val_Acc: 0.590000\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss: 0.004978 Val_Acc: 0.588000\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss: 0.003553 Val_Acc: 0.586000\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss: 0.002156 Val_Acc: 0.592000\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss: 0.001792 Val_Acc: 0.586000\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss: 0.001633 Val_Acc: 0.584000\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss: 0.001486 Val_Acc: 0.602000\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss: 0.001548 Val_Acc: 0.590000\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss: 0.001858 Val_Acc: 0.586000\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss: 0.001513 Val_Acc: 0.590000\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss: 0.001334 Val_Acc: 0.594000\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss: 0.001057 Val_Acc: 0.598000\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss: 0.001314 Val_Acc: 0.576000\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss: 0.001328 Val_Acc: 0.604000\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss: 0.001380 Val_Acc: 0.592000\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss: 0.001055 Val_Acc: 0.588000\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss: 0.001088 Val_Acc: 0.582000\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss: 0.001234 Val_Acc: 0.594000\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss: 0.000869 Val_Acc: 0.604000\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss: 0.001299 Val_Acc: 0.588000\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss: 0.000604 Val_Acc: 0.580000\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss: 0.000583 Val_Acc: 0.600000\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss: 0.000952 Val_Acc: 0.580000\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss: 0.000582 Val_Acc: 0.578000\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss: 0.000592 Val_Acc: 0.596000\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss: 0.000605 Val_Acc: 0.580000\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss: 0.000634 Val_Acc: 0.580000\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss: 0.000433 Val_Acc: 0.594000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss: 2.210034 Val_Acc: 0.194000\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss: 2.274038 Val_Acc: 0.114000\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss: 2.116231 Val_Acc: 0.148000\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss: 2.120804 Val_Acc: 0.206000\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss: 1.980442 Val_Acc: 0.230000\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss: 1.887423 Val_Acc: 0.288000\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss: 1.838786 Val_Acc: 0.322000\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss: 1.684790 Val_Acc: 0.368000\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss: 1.677107 Val_Acc: 0.404000\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss: 1.520665 Val_Acc: 0.448000\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss: 1.431941 Val_Acc: 0.474000\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss: 1.428314 Val_Acc: 0.488000\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss: 1.374901 Val_Acc: 0.470000\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss: 1.423506 Val_Acc: 0.498000\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss: 1.193548 Val_Acc: 0.518000\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss: 1.219189 Val_Acc: 0.548000\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss: 1.245129 Val_Acc: 0.514000\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss: 1.214163 Val_Acc: 0.494000\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss: 1.238075 Val_Acc: 0.558000\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss: 1.019500 Val_Acc: 0.544000\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss: 1.089760 Val_Acc: 0.584000\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss: 1.119346 Val_Acc: 0.556000\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss: 1.060392 Val_Acc: 0.518000\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss: 1.147166 Val_Acc: 0.552000\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss: 0.929813 Val_Acc: 0.576000\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss: 1.002925 Val_Acc: 0.570000\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss: 0.976600 Val_Acc: 0.600000\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss: 0.950944 Val_Acc: 0.546000\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss: 1.047163 Val_Acc: 0.572000\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss: 0.813151 Val_Acc: 0.586000\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss: 0.873378 Val_Acc: 0.606000\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss: 0.881574 Val_Acc: 0.606000\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss: 0.863393 Val_Acc: 0.556000\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss: 1.011027 Val_Acc: 0.568000\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss: 0.739678 Val_Acc: 0.592000\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss: 0.811206 Val_Acc: 0.606000\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss: 0.830747 Val_Acc: 0.608000\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss: 0.769875 Val_Acc: 0.588000\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss: 0.951755 Val_Acc: 0.564000\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss: 0.671517 Val_Acc: 0.604000\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss: 0.688871 Val_Acc: 0.608000\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss: 0.772289 Val_Acc: 0.604000\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss: 0.721900 Val_Acc: 0.600000\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss: 0.887432 Val_Acc: 0.590000\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss: 0.724544 Val_Acc: 0.572000\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss: 0.678991 Val_Acc: 0.620000\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss: 0.755405 Val_Acc: 0.620000\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss: 0.703857 Val_Acc: 0.588000\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss: 0.808707 Val_Acc: 0.608000\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss: 0.557799 Val_Acc: 0.634000\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss: 0.599183 Val_Acc: 0.636000\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss: 0.664739 Val_Acc: 0.622000\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss: 0.561885 Val_Acc: 0.620000\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss: 0.769340 Val_Acc: 0.632000\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss: 0.579021 Val_Acc: 0.626000\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss: 0.559097 Val_Acc: 0.638000\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss: 0.611988 Val_Acc: 0.606000\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss: 0.534006 Val_Acc: 0.626000\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss: 0.703018 Val_Acc: 0.638000\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss: 0.533016 Val_Acc: 0.618000\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss: 0.557077 Val_Acc: 0.644000\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss: 0.578029 Val_Acc: 0.624000\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss: 0.508334 Val_Acc: 0.664000\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss: 0.641688 Val_Acc: 0.644000\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss: 0.457198 Val_Acc: 0.648000\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss: 0.487266 Val_Acc: 0.638000\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss: 0.508766 Val_Acc: 0.624000\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss: 0.451271 Val_Acc: 0.662000\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss: 0.581636 Val_Acc: 0.662000\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss: 0.399872 Val_Acc: 0.674000\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss: 0.437800 Val_Acc: 0.658000\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss: 0.465582 Val_Acc: 0.652000\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss: 0.410900 Val_Acc: 0.670000\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss: 0.524600 Val_Acc: 0.646000\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss: 0.391659 Val_Acc: 0.678000\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss: 0.376907 Val_Acc: 0.660000\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss: 0.412996 Val_Acc: 0.654000\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss: 0.363163 Val_Acc: 0.674000\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss: 0.472037 Val_Acc: 0.674000\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss: 0.342332 Val_Acc: 0.686000\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss: 0.346805 Val_Acc: 0.670000\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss: 0.391630 Val_Acc: 0.690000\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss: 0.320273 Val_Acc: 0.660000\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss: 0.446172 Val_Acc: 0.674000\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss: 0.293767 Val_Acc: 0.678000\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss: 0.332345 Val_Acc: 0.656000\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss: 0.334909 Val_Acc: 0.656000\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss: 0.267141 Val_Acc: 0.668000\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss: 0.385383 Val_Acc: 0.690000\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss: 0.257713 Val_Acc: 0.690000\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss: 0.285261 Val_Acc: 0.656000\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss: 0.306593 Val_Acc: 0.646000\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss: 0.223113 Val_Acc: 0.686000\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss: 0.356392 Val_Acc: 0.670000\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss: 0.229609 Val_Acc: 0.684000\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss: 0.275381 Val_Acc: 0.654000\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss: 0.294425 Val_Acc: 0.656000\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss: 0.203384 Val_Acc: 0.682000\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss: 0.300824 Val_Acc: 0.690000\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss: 0.209697 Val_Acc: 0.678000\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss: 0.266464 Val_Acc: 0.658000\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss: 0.272975 Val_Acc: 0.650000\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss: 0.162075 Val_Acc: 0.674000\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss: 0.257857 Val_Acc: 0.666000\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss: 0.183573 Val_Acc: 0.674000\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss: 0.227855 Val_Acc: 0.672000\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss: 0.211208 Val_Acc: 0.662000\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss: 0.177719 Val_Acc: 0.676000\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss: 0.225795 Val_Acc: 0.688000\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss: 0.170706 Val_Acc: 0.684000\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss: 0.195874 Val_Acc: 0.678000\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss: 0.190914 Val_Acc: 0.676000\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss: 0.137733 Val_Acc: 0.676000\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss: 0.193818 Val_Acc: 0.670000\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss: 0.141026 Val_Acc: 0.680000\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss: 0.217361 Val_Acc: 0.654000\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss: 0.172537 Val_Acc: 0.692000\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss: 0.122191 Val_Acc: 0.678000\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss: 0.167625 Val_Acc: 0.684000\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss: 0.132272 Val_Acc: 0.682000\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss: 0.174726 Val_Acc: 0.674000\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss: 0.143186 Val_Acc: 0.686000\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss: 0.111773 Val_Acc: 0.678000\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss: 0.145471 Val_Acc: 0.686000\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss: 0.127029 Val_Acc: 0.680000\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss: 0.152191 Val_Acc: 0.688000\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss: 0.126983 Val_Acc: 0.694000\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss: 0.077967 Val_Acc: 0.668000\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss: 0.137860 Val_Acc: 0.660000\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss: 0.105362 Val_Acc: 0.686000\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss: 0.139043 Val_Acc: 0.678000\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss: 0.121441 Val_Acc: 0.690000\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss: 0.083970 Val_Acc: 0.694000\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss: 0.133864 Val_Acc: 0.688000\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss: 0.092725 Val_Acc: 0.676000\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss: 0.106483 Val_Acc: 0.696000\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss: 0.098812 Val_Acc: 0.708000\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss: 0.065429 Val_Acc: 0.686000\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss: 0.104426 Val_Acc: 0.686000\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss: 0.074670 Val_Acc: 0.694000\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss: 0.116490 Val_Acc: 0.690000\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss: 0.087483 Val_Acc: 0.690000\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss: 0.059681 Val_Acc: 0.684000\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss: 0.097419 Val_Acc: 0.654000\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss: 0.064350 Val_Acc: 0.696000\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss: 0.086350 Val_Acc: 0.690000\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss: 0.072334 Val_Acc: 0.680000\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss: 0.041708 Val_Acc: 0.690000\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss: 0.098062 Val_Acc: 0.688000\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss: 0.070997 Val_Acc: 0.668000\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss: 0.065529 Val_Acc: 0.706000\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss: 0.064811 Val_Acc: 0.674000\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss: 0.046527 Val_Acc: 0.682000\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss: 0.066490 Val_Acc: 0.682000\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss: 0.057584 Val_Acc: 0.684000\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss: 0.060517 Val_Acc: 0.682000\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss: 0.050729 Val_Acc: 0.688000\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss: 0.038369 Val_Acc: 0.676000\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss: 0.059386 Val_Acc: 0.684000\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss: 0.066131 Val_Acc: 0.658000\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss: 0.061510 Val_Acc: 0.702000\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss: 0.055483 Val_Acc: 0.676000\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss: 0.037435 Val_Acc: 0.676000\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss: 0.054468 Val_Acc: 0.682000\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss: 0.044017 Val_Acc: 0.708000\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss: 0.051467 Val_Acc: 0.708000\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss: 0.043839 Val_Acc: 0.672000\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss: 0.040172 Val_Acc: 0.686000\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss: 0.061116 Val_Acc: 0.684000\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss: 0.034782 Val_Acc: 0.724000\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss: 0.046330 Val_Acc: 0.702000\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss: 0.045655 Val_Acc: 0.676000\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss: 0.033580 Val_Acc: 0.656000\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss: 0.046311 Val_Acc: 0.688000\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss: 0.037434 Val_Acc: 0.714000\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss: 0.036751 Val_Acc: 0.694000\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss: 0.036782 Val_Acc: 0.686000\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss: 0.031520 Val_Acc: 0.684000\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss: 0.038190 Val_Acc: 0.688000\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss: 0.040240 Val_Acc: 0.684000\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss: 0.054315 Val_Acc: 0.704000\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss: 0.043265 Val_Acc: 0.682000\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss: 0.023574 Val_Acc: 0.686000\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss: 0.057180 Val_Acc: 0.688000\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss: 0.055727 Val_Acc: 0.690000\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss: 0.041794 Val_Acc: 0.688000\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss: 0.052780 Val_Acc: 0.676000\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss: 0.029686 Val_Acc: 0.688000\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss: 0.040356 Val_Acc: 0.676000\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss: 0.038417 Val_Acc: 0.680000\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss: 0.032967 Val_Acc: 0.712000\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss: 0.042073 Val_Acc: 0.682000\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss: 0.028207 Val_Acc: 0.696000\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss: 0.048503 Val_Acc: 0.710000\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss: 0.045653 Val_Acc: 0.682000\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss: 0.032516 Val_Acc: 0.700000\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss: 0.038062 Val_Acc: 0.680000\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss: 0.019939 Val_Acc: 0.694000\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss: 0.042644 Val_Acc: 0.688000\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss: 0.026772 Val_Acc: 0.700000\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss: 0.036497 Val_Acc: 0.708000\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss: 0.030699 Val_Acc: 0.664000\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss: 0.026552 Val_Acc: 0.688000\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss: 0.035770 Val_Acc: 0.700000\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss: 0.023428 Val_Acc: 0.688000\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss: 0.027070 Val_Acc: 0.708000\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss: 0.028931 Val_Acc: 0.684000\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss: 0.017704 Val_Acc: 0.684000\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss: 0.034050 Val_Acc: 0.702000\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss: 0.018886 Val_Acc: 0.690000\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss: 0.021724 Val_Acc: 0.706000\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss: 0.018284 Val_Acc: 0.688000\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss: 0.031849 Val_Acc: 0.702000\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss: 0.025353 Val_Acc: 0.690000\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss: 0.020342 Val_Acc: 0.680000\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss: 0.016925 Val_Acc: 0.710000\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss: 0.020066 Val_Acc: 0.676000\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss: 0.022458 Val_Acc: 0.702000\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss: 0.021470 Val_Acc: 0.688000\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss: 0.015357 Val_Acc: 0.684000\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss: 0.015956 Val_Acc: 0.710000\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss: 0.014574 Val_Acc: 0.696000\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss: 0.012374 Val_Acc: 0.688000\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss: 0.016519 Val_Acc: 0.672000\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss: 0.016431 Val_Acc: 0.686000\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss: 0.016954 Val_Acc: 0.698000\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss: 0.018085 Val_Acc: 0.692000\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss: 0.012684 Val_Acc: 0.700000\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss: 0.014306 Val_Acc: 0.664000\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss: 0.015643 Val_Acc: 0.706000\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss: 0.014512 Val_Acc: 0.696000\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss: 0.015671 Val_Acc: 0.696000\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss: 0.019061 Val_Acc: 0.672000\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss: 0.015691 Val_Acc: 0.666000\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss: 0.014360 Val_Acc: 0.686000\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss: 0.015050 Val_Acc: 0.704000\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss: 0.012718 Val_Acc: 0.688000\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss: 0.013155 Val_Acc: 0.690000\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss: 0.027881 Val_Acc: 0.646000\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss: 0.013624 Val_Acc: 0.708000\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss: 0.010109 Val_Acc: 0.706000\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss: 0.012594 Val_Acc: 0.692000\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss: 0.013588 Val_Acc: 0.700000\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss: 0.014274 Val_Acc: 0.684000\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss: 0.015175 Val_Acc: 0.704000\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss: 0.011609 Val_Acc: 0.698000\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss: 0.007857 Val_Acc: 0.708000\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss: 0.011520 Val_Acc: 0.700000\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss: 0.010038 Val_Acc: 0.690000\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss: 0.007032 Val_Acc: 0.708000\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss: 0.007811 Val_Acc: 0.700000\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss: 0.008087 Val_Acc: 0.704000\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss: 0.011508 Val_Acc: 0.702000\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss: 0.015989 Val_Acc: 0.674000\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss: 0.009389 Val_Acc: 0.680000\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss: 0.010374 Val_Acc: 0.702000\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss: 0.008060 Val_Acc: 0.706000\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss: 0.009448 Val_Acc: 0.696000\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss: 0.015671 Val_Acc: 0.686000\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss: 0.008741 Val_Acc: 0.694000\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss: 0.013461 Val_Acc: 0.694000\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss: 0.006021 Val_Acc: 0.700000\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss: 0.011150 Val_Acc: 0.688000\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss: 0.012554 Val_Acc: 0.700000\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss: 0.009813 Val_Acc: 0.674000\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss: 0.006476 Val_Acc: 0.708000\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss: 0.004994 Val_Acc: 0.690000\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss: 0.007552 Val_Acc: 0.686000\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss: 0.011301 Val_Acc: 0.694000\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss: 0.006170 Val_Acc: 0.708000\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss: 0.007578 Val_Acc: 0.706000\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss: 0.006470 Val_Acc: 0.706000\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss: 0.007500 Val_Acc: 0.690000\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss: 0.013278 Val_Acc: 0.688000\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss: 0.009897 Val_Acc: 0.688000\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss: 0.007895 Val_Acc: 0.696000\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss: 0.004549 Val_Acc: 0.704000\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss: 0.006231 Val_Acc: 0.696000\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss: 0.007379 Val_Acc: 0.688000\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss: 0.007458 Val_Acc: 0.670000\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss: 0.008611 Val_Acc: 0.704000\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss: 0.005585 Val_Acc: 0.700000\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss: 0.006916 Val_Acc: 0.682000\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss: 0.010664 Val_Acc: 0.678000\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss: 0.008479 Val_Acc: 0.666000\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss: 0.009040 Val_Acc: 0.710000\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss: 0.007652 Val_Acc: 0.680000\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss: 0.014499 Val_Acc: 0.674000\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss: 0.012808 Val_Acc: 0.688000\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss: 0.004844 Val_Acc: 0.682000\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss: 0.007218 Val_Acc: 0.710000\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss: 0.010725 Val_Acc: 0.688000\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss: 0.011132 Val_Acc: 0.686000\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss: 0.005073 Val_Acc: 0.700000\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss: 0.004963 Val_Acc: 0.670000\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss: 0.008132 Val_Acc: 0.718000\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss: 0.006222 Val_Acc: 0.692000\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss: 0.007482 Val_Acc: 0.682000\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss: 0.006770 Val_Acc: 0.690000\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss: 0.005664 Val_Acc: 0.694000\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss: 0.005748 Val_Acc: 0.694000\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss: 0.005849 Val_Acc: 0.690000\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss: 0.005844 Val_Acc: 0.674000\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss: 0.006410 Val_Acc: 0.694000\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss: 0.004759 Val_Acc: 0.688000\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss: 0.007801 Val_Acc: 0.700000\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss: 0.003715 Val_Acc: 0.696000\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss: 0.003803 Val_Acc: 0.672000\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss: 0.006264 Val_Acc: 0.690000\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss: 0.005049 Val_Acc: 0.694000\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss: 0.006989 Val_Acc: 0.684000\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss: 0.004890 Val_Acc: 0.696000\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss: 0.003668 Val_Acc: 0.690000\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss: 0.004785 Val_Acc: 0.704000\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss: 0.005374 Val_Acc: 0.700000\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss: 0.005142 Val_Acc: 0.682000\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss: 0.004064 Val_Acc: 0.718000\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss: 0.003768 Val_Acc: 0.688000\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss: 0.005354 Val_Acc: 0.692000\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss: 0.002435 Val_Acc: 0.706000\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss: 0.004823 Val_Acc: 0.702000\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss: 0.006288 Val_Acc: 0.698000\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss: 0.002583 Val_Acc: 0.684000\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss: 0.018984 Val_Acc: 0.708000\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss: 0.004085 Val_Acc: 0.688000\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss: 0.003023 Val_Acc: 0.704000\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss: 0.002557 Val_Acc: 0.706000\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss: 0.003351 Val_Acc: 0.696000\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss: 0.003910 Val_Acc: 0.698000\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss: 0.003320 Val_Acc: 0.698000\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss: 0.002183 Val_Acc: 0.682000\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss: 0.005514 Val_Acc: 0.698000\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss: 0.003181 Val_Acc: 0.694000\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss: 0.004110 Val_Acc: 0.692000\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss: 0.003119 Val_Acc: 0.722000\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss: 0.003079 Val_Acc: 0.696000\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss: 0.001887 Val_Acc: 0.696000\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss: 0.004234 Val_Acc: 0.708000\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss: 0.004956 Val_Acc: 0.692000\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss: 0.005111 Val_Acc: 0.692000\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss: 0.002351 Val_Acc: 0.676000\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss: 0.004289 Val_Acc: 0.700000\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss: 0.003483 Val_Acc: 0.696000\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss: 0.006562 Val_Acc: 0.706000\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss: 0.003067 Val_Acc: 0.724000\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss: 0.002609 Val_Acc: 0.702000\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss: 0.004535 Val_Acc: 0.696000\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss: 0.002878 Val_Acc: 0.696000\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss: 0.004705 Val_Acc: 0.712000\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss: 0.001902 Val_Acc: 0.708000\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss: 0.002965 Val_Acc: 0.688000\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss: 0.003474 Val_Acc: 0.708000\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss: 0.006222 Val_Acc: 0.724000\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss: 0.006016 Val_Acc: 0.700000\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss: 0.002228 Val_Acc: 0.732000\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss: 0.002166 Val_Acc: 0.698000\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss: 0.002980 Val_Acc: 0.706000\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss: 0.003756 Val_Acc: 0.702000\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss: 0.002380 Val_Acc: 0.720000\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss: 0.001680 Val_Acc: 0.700000\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss: 0.002924 Val_Acc: 0.700000\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss: 0.001798 Val_Acc: 0.692000\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss: 0.002633 Val_Acc: 0.698000\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss: 0.001779 Val_Acc: 0.696000\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss: 0.001450 Val_Acc: 0.714000\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss: 0.004439 Val_Acc: 0.666000\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss: 0.002979 Val_Acc: 0.714000\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss: 0.002040 Val_Acc: 0.692000\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss: 0.002656 Val_Acc: 0.708000\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss: 0.002847 Val_Acc: 0.714000\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss: 0.003664 Val_Acc: 0.692000\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss: 0.003029 Val_Acc: 0.706000\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss: 0.002178 Val_Acc: 0.694000\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss: 0.002566 Val_Acc: 0.708000\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss: 0.002881 Val_Acc: 0.706000\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss: 0.004117 Val_Acc: 0.682000\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss: 0.003133 Val_Acc: 0.716000\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss: 0.002761 Val_Acc: 0.712000\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss: 0.003181 Val_Acc: 0.712000\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss: 0.003208 Val_Acc: 0.710000\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss: 0.003815 Val_Acc: 0.678000\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss: 0.002358 Val_Acc: 0.724000\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss: 0.001253 Val_Acc: 0.682000\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss: 0.003366 Val_Acc: 0.702000\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss: 0.002903 Val_Acc: 0.690000\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss: 0.001473 Val_Acc: 0.692000\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss: 0.002826 Val_Acc: 0.710000\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss: 0.002513 Val_Acc: 0.690000\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss: 0.004941 Val_Acc: 0.712000\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss: 0.001544 Val_Acc: 0.706000\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss: 0.001820 Val_Acc: 0.696000\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss: 0.002330 Val_Acc: 0.702000\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss: 0.002362 Val_Acc: 0.708000\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss: 0.002364 Val_Acc: 0.710000\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss: 0.003098 Val_Acc: 0.698000\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss: 0.001946 Val_Acc: 0.698000\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss: 0.001229 Val_Acc: 0.694000\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss: 0.002185 Val_Acc: 0.702000\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss: 0.002263 Val_Acc: 0.708000\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss: 0.002109 Val_Acc: 0.706000\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss: 0.000975 Val_Acc: 0.702000\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss: 0.002217 Val_Acc: 0.688000\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss: 0.002098 Val_Acc: 0.702000\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss: 0.001819 Val_Acc: 0.702000\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss: 0.001968 Val_Acc: 0.692000\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss: 0.003313 Val_Acc: 0.706000\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss: 0.001638 Val_Acc: 0.700000\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss: 0.004691 Val_Acc: 0.696000\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss: 0.002318 Val_Acc: 0.698000\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss: 0.001380 Val_Acc: 0.686000\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss: 0.001523 Val_Acc: 0.696000\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss: 0.003167 Val_Acc: 0.684000\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss: 0.005441 Val_Acc: 0.708000\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss: 0.002064 Val_Acc: 0.706000\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss: 0.002167 Val_Acc: 0.700000\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss: 0.001337 Val_Acc: 0.690000\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss: 0.002798 Val_Acc: 0.702000\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss: 0.001598 Val_Acc: 0.700000\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss: 0.002208 Val_Acc: 0.716000\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss: 0.002264 Val_Acc: 0.696000\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss: 0.000730 Val_Acc: 0.714000\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss: 0.001525 Val_Acc: 0.698000\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss: 0.003432 Val_Acc: 0.704000\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss: 0.003014 Val_Acc: 0.710000\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss: 0.001393 Val_Acc: 0.698000\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss: 0.000960 Val_Acc: 0.696000\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss: 0.002270 Val_Acc: 0.694000\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss: 0.002181 Val_Acc: 0.698000\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss: 0.004245 Val_Acc: 0.704000\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss: 0.002293 Val_Acc: 0.698000\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss: 0.002951 Val_Acc: 0.718000\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss: 0.000429 Val_Acc: 0.704000\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss: 0.001389 Val_Acc: 0.696000\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss: 0.002905 Val_Acc: 0.694000\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss: 0.002955 Val_Acc: 0.706000\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss: 0.001260 Val_Acc: 0.704000\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss: 0.000641 Val_Acc: 0.714000\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss: 0.001558 Val_Acc: 0.702000\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss: 0.003421 Val_Acc: 0.696000\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss: 0.001194 Val_Acc: 0.704000\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss: 0.001979 Val_Acc: 0.694000\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss: 0.002201 Val_Acc: 0.692000\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss: 0.001624 Val_Acc: 0.700000\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss: 0.001486 Val_Acc: 0.700000\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss: 0.001140 Val_Acc: 0.702000\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss: 0.001160 Val_Acc: 0.710000\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss: 0.001935 Val_Acc: 0.690000\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss: 0.001231 Val_Acc: 0.690000\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss: 0.001757 Val_Acc: 0.714000\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss: 0.001851 Val_Acc: 0.702000\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss: 0.002358 Val_Acc: 0.694000\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss: 0.001492 Val_Acc: 0.694000\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss: 0.001394 Val_Acc: 0.702000\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss: 0.003596 Val_Acc: 0.686000\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss: 0.001655 Val_Acc: 0.706000\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss: 0.001515 Val_Acc: 0.718000\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss: 0.001935 Val_Acc: 0.704000\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss: 0.001551 Val_Acc: 0.700000\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss: 0.000815 Val_Acc: 0.692000\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss: 0.001834 Val_Acc: 0.696000\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss: 0.002253 Val_Acc: 0.706000\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss: 0.000837 Val_Acc: 0.702000\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss: 0.002176 Val_Acc: 0.722000\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss: 0.001540 Val_Acc: 0.708000\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss: 0.004654 Val_Acc: 0.706000\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss: 0.001771 Val_Acc: 0.690000\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss: 0.002929 Val_Acc: 0.708000\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss: 0.001237 Val_Acc: 0.706000\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss: 0.001445 Val_Acc: 0.694000\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss: 0.001265 Val_Acc: 0.700000\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss: 0.003089 Val_Acc: 0.712000\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss: 0.001063 Val_Acc: 0.692000\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss: 0.001759 Val_Acc: 0.708000\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss: 0.001193 Val_Acc: 0.718000\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss: 0.001132 Val_Acc: 0.710000\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss: 0.001034 Val_Acc: 0.714000\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss: 0.001974 Val_Acc: 0.706000\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss: 0.001069 Val_Acc: 0.704000\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss: 0.002237 Val_Acc: 0.710000\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss: 0.000901 Val_Acc: 0.694000\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss: 0.000891 Val_Acc: 0.726000\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss: 0.001453 Val_Acc: 0.712000\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss: 0.001733 Val_Acc: 0.706000\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss: 0.001704 Val_Acc: 0.698000\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss: 0.000673 Val_Acc: 0.698000\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss: 0.003198 Val_Acc: 0.694000\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss: 0.000886 Val_Acc: 0.720000\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss: 0.003771 Val_Acc: 0.710000\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss: 0.000807 Val_Acc: 0.700000\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss: 0.001178 Val_Acc: 0.710000\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss: 0.000771 Val_Acc: 0.696000\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss: 0.001500 Val_Acc: 0.692000\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss: 0.001865 Val_Acc: 0.698000\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss: 0.001458 Val_Acc: 0.710000\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss: 0.001020 Val_Acc: 0.702000\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss: 0.001936 Val_Acc: 0.690000\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss: 0.000901 Val_Acc: 0.706000\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss: 0.001642 Val_Acc: 0.726000\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss: 0.000685 Val_Acc: 0.702000\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss: 0.000575 Val_Acc: 0.700000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.6567842364311218\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArwAAAJ/CAYAAACaxgBZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVIJXRERERHLt/wEtCQou9gFw+wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x191eb7a66a0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 350
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-70% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 70%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

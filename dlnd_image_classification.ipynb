{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bf549d7d68>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return x / 255\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    temp = np.zeros([len(x), 10])\n",
    "    temp[range(len(x)), x] = 1        \n",
    "\n",
    "    return temp\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    "If you're finding it hard to dedicate enough time for this course a week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) to build each layer, except \"Convolutional & Max Pooling\" layer.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    "If you would like to get the most of this course, try to solve all the problems without TF Layers.  Let's begin!\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a bach of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    shp = (None, ) + image_shape\n",
    "    x = tf.placeholder(tf.float32, shape=shp, name='x')\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.int32, shape=(None, n_classes), name='y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "Note: You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for this layer.  You're free to use any TensorFlow package for all the other layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    #print(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    depth = int(x_tensor.shape[3])\n",
    "    weight = tf.Variable(tf.truncated_normal([conv_ksize[0], conv_ksize[1], depth, conv_num_outputs]))\n",
    "    bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    \n",
    "    conv_layer = tf.nn.conv2d(x_tensor, weight, strides=[1, conv_strides[0], conv_strides[1], 1], padding='SAME')\n",
    "    conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    \n",
    "    pool_layer = tf.nn.max_pool(conv_layer, [1, pool_ksize[0], pool_ksize[1], 1], [1, pool_strides[0], pool_strides[1], 1], \n",
    "                                'SAME')\n",
    "    return pool_layer \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). You can use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    size = int(x_tensor.shape[1]) * int(x_tensor.shape[2]) * int(x_tensor.shape[3])\n",
    "    rv = tf.reshape(x_tensor, [-1, size])\n",
    "    \n",
    "    return rv\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). You can use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    shape = int(x_tensor.shape[1])\n",
    "    weight = tf.Variable(tf.truncated_normal([shape, num_outputs]))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    dense = tf.matmul(x_tensor, weight)\n",
    "    dense = tf.add(dense, bias)\n",
    "    \n",
    "    return dense\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). You can use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for this layer.\n",
    "\n",
    "Note: Activation, softmax, or cross entropy shouldn't be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return fully_conn(x_tensor, num_outputs)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "    depth = 10\n",
    "    conv_ksize = 4\n",
    "    conv_stride = 1\n",
    "    pool_ksize = 4\n",
    "    net = conv2d_maxpool(x, depth, (conv_ksize, conv_ksize), (conv_stride, conv_stride), \n",
    "                         (pool_ksize, pool_ksize), (pool_ksize, pool_ksize))\n",
    "    \n",
    "    depth = 50\n",
    "    conv_ksize = 2\n",
    "    conv_stride = 1\n",
    "    pool_ksize = 2\n",
    "    net = conv2d_maxpool(x, depth, (conv_ksize, conv_ksize), (conv_stride, conv_stride), \n",
    "                         (pool_ksize, pool_ksize), (pool_ksize, pool_ksize))\n",
    "    \n",
    "    depth = 100\n",
    "    conv_ksize = 2\n",
    "    conv_stride = 1\n",
    "    pool_ksize = 2\n",
    "    net = conv2d_maxpool(x, depth, (conv_ksize, conv_ksize), (conv_stride, conv_stride), \n",
    "                        (pool_ksize, pool_ksize), (pool_ksize, pool_ksize))\n",
    "    \n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    net = flatten(net)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    \n",
    "    net = fully_conn(net, 30)\n",
    "    net = tf.nn.dropout(net, keep_prob)\n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    \n",
    "    net = output(net, 10)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return net\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer, feed_dict={\n",
    "                x: feature_batch,\n",
    "                y: label_batch,\n",
    "                keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    loss = session.run(cost, feed_dict={\n",
    "                x: feature_batch,\n",
    "                y: label_batch,\n",
    "                keep_prob: 1.})\n",
    "    valid_acc = session.run(accuracy, feed_dict={\n",
    "                x: feature_batch,\n",
    "                y: label_batch,\n",
    "                keep_prob: 1.})\n",
    "    print('Loss: {:.6f} Val_Acc: {:.6f}'.format(\n",
    "                loss,\n",
    "                valid_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 100\n",
    "batch_size = 256\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss: 240.655609 Val_Acc: 0.200000\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss: 196.911942 Val_Acc: 0.225000\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss: 161.423645 Val_Acc: 0.350000\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss: 138.940094 Val_Acc: 0.350000\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss: 119.769287 Val_Acc: 0.425000\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss: 99.508553 Val_Acc: 0.425000\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss: 86.967468 Val_Acc: 0.425000\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss: 75.540344 Val_Acc: 0.425000\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss: 62.842926 Val_Acc: 0.450000\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss: 56.906597 Val_Acc: 0.475000\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss: 44.301239 Val_Acc: 0.500000\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss: 41.925510 Val_Acc: 0.500000\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss: 38.302345 Val_Acc: 0.550000\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss: 33.899643 Val_Acc: 0.625000\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss: 32.566338 Val_Acc: 0.625000\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss: 34.161201 Val_Acc: 0.625000\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss: 27.718161 Val_Acc: 0.700000\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss: 27.185249 Val_Acc: 0.650000\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss: 22.345207 Val_Acc: 0.675000\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss: 20.175755 Val_Acc: 0.675000\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss: 18.677462 Val_Acc: 0.725000\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss: 17.511276 Val_Acc: 0.675000\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss: 16.952663 Val_Acc: 0.700000\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss: 14.263102 Val_Acc: 0.700000\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss: 13.371222 Val_Acc: 0.725000\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss: 11.847982 Val_Acc: 0.725000\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss: 11.726343 Val_Acc: 0.750000\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss: 12.062136 Val_Acc: 0.800000\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss: 11.288551 Val_Acc: 0.750000\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss: 10.292356 Val_Acc: 0.775000\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss: 9.877966 Val_Acc: 0.750000\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss: 7.986477 Val_Acc: 0.775000\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss: 8.368259 Val_Acc: 0.850000\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss: 7.708127 Val_Acc: 0.825000\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss: 7.152337 Val_Acc: 0.850000\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss: 6.355209 Val_Acc: 0.825000\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss: 6.135617 Val_Acc: 0.825000\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss: 6.241608 Val_Acc: 0.850000\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss: 5.917027 Val_Acc: 0.875000\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss: 5.547791 Val_Acc: 0.850000\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss: 5.182826 Val_Acc: 0.725000\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss: 4.741402 Val_Acc: 0.850000\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss: 4.512930 Val_Acc: 0.825000\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss: 3.809613 Val_Acc: 0.850000\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss: 3.828016 Val_Acc: 0.850000\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss: 3.781077 Val_Acc: 0.825000\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss: 3.261779 Val_Acc: 0.850000\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss: 3.368295 Val_Acc: 0.850000\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss: 2.629431 Val_Acc: 0.875000\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss: 2.637203 Val_Acc: 0.775000\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss: 2.292615 Val_Acc: 0.875000\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss: 2.310624 Val_Acc: 0.800000\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss: 2.195529 Val_Acc: 0.750000\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss: 2.050863 Val_Acc: 0.775000\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss: 2.025727 Val_Acc: 0.850000\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss: 1.581541 Val_Acc: 0.850000\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss: 1.999014 Val_Acc: 0.825000\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss: 1.716922 Val_Acc: 0.825000\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss: 1.478822 Val_Acc: 0.800000\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss: 1.281385 Val_Acc: 0.825000\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss: 1.178473 Val_Acc: 0.800000\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss: 1.360913 Val_Acc: 0.850000\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss: 0.910932 Val_Acc: 0.775000\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss: 0.866005 Val_Acc: 0.800000\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss: 0.900135 Val_Acc: 0.825000\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss: 0.670132 Val_Acc: 0.850000\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss: 0.749718 Val_Acc: 0.825000\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss: 0.821589 Val_Acc: 0.825000\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss: 0.505597 Val_Acc: 0.800000\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss: 0.974743 Val_Acc: 0.800000\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss: 0.484894 Val_Acc: 0.800000\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss: 0.309086 Val_Acc: 0.850000\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss: 0.473695 Val_Acc: 0.825000\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss: 0.574251 Val_Acc: 0.850000\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss: 0.412197 Val_Acc: 0.825000\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss: 0.461180 Val_Acc: 0.825000\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss: 0.383840 Val_Acc: 0.825000\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss: 0.402479 Val_Acc: 0.850000\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss: 0.490047 Val_Acc: 0.825000\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss: 0.572211 Val_Acc: 0.850000\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss: 0.478704 Val_Acc: 0.850000\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss: 0.385185 Val_Acc: 0.825000\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss: 0.483063 Val_Acc: 0.850000\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss: 0.432009 Val_Acc: 0.875000\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss: 0.506794 Val_Acc: 0.825000\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss: 0.605827 Val_Acc: 0.825000\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss: 0.612812 Val_Acc: 0.825000\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss: 0.652509 Val_Acc: 0.825000\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss: 0.601862 Val_Acc: 0.825000\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss: 0.576586 Val_Acc: 0.825000\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss: 0.581432 Val_Acc: 0.850000\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss: 0.570274 Val_Acc: 0.825000\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss: 0.579173 Val_Acc: 0.850000\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss: 0.591833 Val_Acc: 0.850000\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss: 0.558376 Val_Acc: 0.850000\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss: 0.616691 Val_Acc: 0.800000\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss: 0.568401 Val_Acc: 0.850000\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss: 0.566651 Val_Acc: 0.875000\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss: 0.561710 Val_Acc: 0.825000\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss: 0.566102 Val_Acc: 0.850000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss: 278.069611 Val_Acc: 0.125000\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss: 200.079102 Val_Acc: 0.200000\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss: 151.331818 Val_Acc: 0.425000\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss: 160.214294 Val_Acc: 0.175000\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss: 145.757446 Val_Acc: 0.250000\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss: 119.621750 Val_Acc: 0.350000\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss: 113.181061 Val_Acc: 0.375000\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss: 92.207642 Val_Acc: 0.475000\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss: 93.532112 Val_Acc: 0.250000\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss: 91.121910 Val_Acc: 0.300000\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss: 80.685669 Val_Acc: 0.375000\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss: 65.172226 Val_Acc: 0.400000\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss: 56.844627 Val_Acc: 0.525000\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss: 64.248657 Val_Acc: 0.425000\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss: 56.644836 Val_Acc: 0.425000\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss: 58.176460 Val_Acc: 0.475000\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss: 43.798885 Val_Acc: 0.450000\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss: 41.115891 Val_Acc: 0.575000\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss: 44.940121 Val_Acc: 0.450000\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss: 42.136925 Val_Acc: 0.500000\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss: 40.619377 Val_Acc: 0.475000\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss: 32.434647 Val_Acc: 0.500000\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss: 31.276648 Val_Acc: 0.600000\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss: 33.210251 Val_Acc: 0.475000\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss: 28.947998 Val_Acc: 0.500000\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss: 31.790592 Val_Acc: 0.525000\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss: 24.238991 Val_Acc: 0.600000\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss: 23.768671 Val_Acc: 0.550000\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss: 26.525181 Val_Acc: 0.500000\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss: 22.552126 Val_Acc: 0.500000\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss: 27.143291 Val_Acc: 0.500000\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss: 20.700111 Val_Acc: 0.550000\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss: 19.753897 Val_Acc: 0.625000\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss: 20.906897 Val_Acc: 0.475000\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss: 19.725700 Val_Acc: 0.550000\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss: 20.708651 Val_Acc: 0.450000\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss: 16.093044 Val_Acc: 0.625000\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss: 15.370596 Val_Acc: 0.650000\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss: 17.031637 Val_Acc: 0.525000\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss: 13.773947 Val_Acc: 0.625000\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss: 15.640045 Val_Acc: 0.450000\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss: 14.341919 Val_Acc: 0.625000\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss: 11.890078 Val_Acc: 0.625000\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss: 13.202136 Val_Acc: 0.625000\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss: 11.491598 Val_Acc: 0.575000\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss: 13.508965 Val_Acc: 0.475000\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss: 11.672869 Val_Acc: 0.625000\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss: 7.383049 Val_Acc: 0.675000\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss: 11.522743 Val_Acc: 0.550000\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss: 8.232002 Val_Acc: 0.600000\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss: 9.953161 Val_Acc: 0.525000\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss: 9.662729 Val_Acc: 0.600000\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss: 5.262870 Val_Acc: 0.650000\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss: 9.196384 Val_Acc: 0.550000\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss: 5.997744 Val_Acc: 0.600000\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss: 8.335269 Val_Acc: 0.525000\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss: 8.535593 Val_Acc: 0.575000\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss: 4.088432 Val_Acc: 0.675000\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss: 7.680440 Val_Acc: 0.575000\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss: 4.646638 Val_Acc: 0.550000\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss: 6.153339 Val_Acc: 0.500000\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss: 8.070407 Val_Acc: 0.550000\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss: 3.063986 Val_Acc: 0.675000\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss: 6.298638 Val_Acc: 0.625000\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss: 4.391743 Val_Acc: 0.575000\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss: 4.706740 Val_Acc: 0.475000\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss: 6.291531 Val_Acc: 0.575000\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss: 2.623681 Val_Acc: 0.625000\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss: 5.481304 Val_Acc: 0.575000\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss: 4.048379 Val_Acc: 0.625000\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss: 4.388134 Val_Acc: 0.525000\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss: 5.241569 Val_Acc: 0.575000\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss: 1.970144 Val_Acc: 0.675000\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss: 4.318369 Val_Acc: 0.575000\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss: 3.429526 Val_Acc: 0.625000\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss: 3.751001 Val_Acc: 0.525000\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss: 4.467387 Val_Acc: 0.550000\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss: 1.757501 Val_Acc: 0.650000\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss: 3.616136 Val_Acc: 0.600000\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss: 2.832688 Val_Acc: 0.575000\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss: 3.139743 Val_Acc: 0.525000\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss: 3.347804 Val_Acc: 0.475000\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss: 1.453267 Val_Acc: 0.625000\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss: 3.108475 Val_Acc: 0.625000\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss: 2.557747 Val_Acc: 0.600000\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss: 2.645537 Val_Acc: 0.525000\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss: 2.763303 Val_Acc: 0.525000\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss: 1.250658 Val_Acc: 0.650000\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss: 2.787814 Val_Acc: 0.625000\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss: 2.072030 Val_Acc: 0.525000\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss: 2.291870 Val_Acc: 0.475000\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss: 2.320130 Val_Acc: 0.500000\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss: 1.209268 Val_Acc: 0.625000\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss: 2.611791 Val_Acc: 0.575000\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss: 1.829769 Val_Acc: 0.525000\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss: 1.961810 Val_Acc: 0.450000\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss: 1.946270 Val_Acc: 0.450000\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss: 1.110334 Val_Acc: 0.650000\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss: 2.517460 Val_Acc: 0.575000\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss: 1.611205 Val_Acc: 0.525000\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss: 1.713183 Val_Acc: 0.525000\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss: 1.703282 Val_Acc: 0.475000\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss: 1.051688 Val_Acc: 0.675000\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss: 2.361521 Val_Acc: 0.625000\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss: 1.514014 Val_Acc: 0.525000\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss: 1.593979 Val_Acc: 0.575000\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss: 1.460532 Val_Acc: 0.475000\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss: 1.087031 Val_Acc: 0.675000\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss: 2.166224 Val_Acc: 0.525000\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss: 1.444026 Val_Acc: 0.550000\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss: 1.496301 Val_Acc: 0.575000\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss: 1.321617 Val_Acc: 0.500000\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss: 1.085788 Val_Acc: 0.650000\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss: 1.962567 Val_Acc: 0.600000\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss: 1.413989 Val_Acc: 0.625000\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss: 1.380494 Val_Acc: 0.600000\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss: 1.293778 Val_Acc: 0.575000\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss: 1.085903 Val_Acc: 0.675000\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss: 1.797890 Val_Acc: 0.575000\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss: 1.335419 Val_Acc: 0.650000\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss: 1.359381 Val_Acc: 0.625000\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss: 1.277608 Val_Acc: 0.550000\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss: 1.071602 Val_Acc: 0.725000\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss: 1.598177 Val_Acc: 0.550000\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss: 1.268542 Val_Acc: 0.650000\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss: 1.309845 Val_Acc: 0.625000\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss: 1.298981 Val_Acc: 0.525000\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss: 1.083671 Val_Acc: 0.700000\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss: 1.469291 Val_Acc: 0.550000\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss: 1.242243 Val_Acc: 0.700000\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss: 1.314209 Val_Acc: 0.600000\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss: 1.296664 Val_Acc: 0.550000\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss: 1.090174 Val_Acc: 0.725000\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss: 1.397177 Val_Acc: 0.550000\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss: 1.248071 Val_Acc: 0.675000\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss: 1.307612 Val_Acc: 0.650000\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss: 1.303766 Val_Acc: 0.600000\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss: 1.079367 Val_Acc: 0.800000\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss: 1.375196 Val_Acc: 0.550000\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss: 1.236370 Val_Acc: 0.675000\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss: 1.320571 Val_Acc: 0.625000\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss: 1.312391 Val_Acc: 0.575000\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss: 1.095329 Val_Acc: 0.700000\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss: 1.357378 Val_Acc: 0.525000\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss: 1.222678 Val_Acc: 0.675000\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss: 1.325124 Val_Acc: 0.625000\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss: 1.309987 Val_Acc: 0.575000\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss: 1.094150 Val_Acc: 0.750000\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss: 1.326550 Val_Acc: 0.600000\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss: 1.220024 Val_Acc: 0.700000\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss: 1.304177 Val_Acc: 0.625000\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss: 1.260526 Val_Acc: 0.600000\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss: 1.072703 Val_Acc: 0.775000\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss: 1.356159 Val_Acc: 0.575000\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss: 1.189368 Val_Acc: 0.750000\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss: 1.298051 Val_Acc: 0.650000\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss: 1.269595 Val_Acc: 0.625000\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss: 1.057669 Val_Acc: 0.800000\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss: 1.310065 Val_Acc: 0.575000\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss: 1.178524 Val_Acc: 0.750000\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss: 1.287675 Val_Acc: 0.600000\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss: 1.247797 Val_Acc: 0.600000\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss: 1.038849 Val_Acc: 0.725000\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss: 1.304798 Val_Acc: 0.575000\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss: 1.166480 Val_Acc: 0.775000\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss: 1.280623 Val_Acc: 0.650000\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss: 1.231181 Val_Acc: 0.650000\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss: 1.030683 Val_Acc: 0.750000\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss: 1.277466 Val_Acc: 0.650000\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss: 1.174372 Val_Acc: 0.775000\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss: 1.262508 Val_Acc: 0.600000\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss: 1.177943 Val_Acc: 0.650000\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss: 1.019265 Val_Acc: 0.800000\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss: 1.253765 Val_Acc: 0.600000\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss: 1.144286 Val_Acc: 0.800000\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss: 1.247413 Val_Acc: 0.600000\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss: 1.193979 Val_Acc: 0.650000\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss: 1.010291 Val_Acc: 0.800000\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss: 1.244906 Val_Acc: 0.575000\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss: 1.156400 Val_Acc: 0.750000\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss: 1.252328 Val_Acc: 0.600000\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss: 1.144208 Val_Acc: 0.675000\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss: 1.009068 Val_Acc: 0.775000\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss: 1.236881 Val_Acc: 0.575000\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss: 1.126624 Val_Acc: 0.775000\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss: 1.240670 Val_Acc: 0.625000\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss: 1.149826 Val_Acc: 0.675000\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss: 0.989153 Val_Acc: 0.775000\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss: 1.207812 Val_Acc: 0.650000\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss: 1.125448 Val_Acc: 0.775000\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss: 1.223691 Val_Acc: 0.600000\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss: 1.127602 Val_Acc: 0.675000\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss: 0.987427 Val_Acc: 0.725000\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss: 1.186427 Val_Acc: 0.675000\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss: 1.102262 Val_Acc: 0.775000\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss: 1.216661 Val_Acc: 0.625000\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss: 1.086657 Val_Acc: 0.650000\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss: 0.963854 Val_Acc: 0.800000\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss: 1.157264 Val_Acc: 0.725000\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss: 1.099116 Val_Acc: 0.750000\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss: 1.188976 Val_Acc: 0.625000\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss: 1.096120 Val_Acc: 0.625000\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss: 0.947649 Val_Acc: 0.775000\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss: 1.151257 Val_Acc: 0.675000\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss: 1.084520 Val_Acc: 0.775000\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss: 1.185052 Val_Acc: 0.625000\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss: 1.045553 Val_Acc: 0.725000\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss: 0.936257 Val_Acc: 0.825000\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss: 1.157652 Val_Acc: 0.675000\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss: 1.069774 Val_Acc: 0.800000\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss: 1.169559 Val_Acc: 0.675000\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss: 1.021381 Val_Acc: 0.650000\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss: 0.903052 Val_Acc: 0.800000\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss: 1.118906 Val_Acc: 0.675000\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss: 1.034436 Val_Acc: 0.800000\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss: 1.131589 Val_Acc: 0.650000\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss: 1.019437 Val_Acc: 0.700000\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss: 0.904132 Val_Acc: 0.750000\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss: 1.115839 Val_Acc: 0.650000\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss: 1.032767 Val_Acc: 0.800000\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss: 1.131306 Val_Acc: 0.600000\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss: 1.004337 Val_Acc: 0.675000\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss: 0.880904 Val_Acc: 0.875000\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss: 1.100537 Val_Acc: 0.675000\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss: 1.007987 Val_Acc: 0.800000\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss: 1.111211 Val_Acc: 0.625000\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss: 0.998770 Val_Acc: 0.675000\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss: 0.870131 Val_Acc: 0.800000\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss: 1.075648 Val_Acc: 0.700000\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss: 1.002622 Val_Acc: 0.800000\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss: 1.088213 Val_Acc: 0.675000\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss: 0.964007 Val_Acc: 0.725000\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss: 0.871691 Val_Acc: 0.800000\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss: 1.051759 Val_Acc: 0.725000\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss: 0.980192 Val_Acc: 0.750000\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss: 1.087699 Val_Acc: 0.625000\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss: 0.923960 Val_Acc: 0.750000\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss: 0.843122 Val_Acc: 0.825000\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss: 1.022927 Val_Acc: 0.725000\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss: 0.980453 Val_Acc: 0.750000\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss: 1.076124 Val_Acc: 0.625000\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss: 0.891508 Val_Acc: 0.775000\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss: 0.806687 Val_Acc: 0.900000\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss: 1.013669 Val_Acc: 0.750000\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss: 0.951916 Val_Acc: 0.800000\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss: 1.085933 Val_Acc: 0.650000\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss: 0.885272 Val_Acc: 0.775000\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss: 0.799588 Val_Acc: 0.775000\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss: 0.976145 Val_Acc: 0.650000\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss: 0.939305 Val_Acc: 0.800000\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss: 1.051987 Val_Acc: 0.700000\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss: 0.886209 Val_Acc: 0.775000\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss: 0.771082 Val_Acc: 0.875000\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss: 0.951859 Val_Acc: 0.700000\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss: 0.934490 Val_Acc: 0.800000\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss: 1.027794 Val_Acc: 0.675000\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss: 0.846491 Val_Acc: 0.825000\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss: 0.748240 Val_Acc: 0.900000\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss: 0.930147 Val_Acc: 0.750000\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss: 0.881557 Val_Acc: 0.800000\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss: 1.036922 Val_Acc: 0.700000\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss: 0.828044 Val_Acc: 0.775000\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss: 0.707173 Val_Acc: 0.900000\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss: 0.883929 Val_Acc: 0.775000\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss: 0.907361 Val_Acc: 0.800000\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss: 1.029984 Val_Acc: 0.725000\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss: 0.826531 Val_Acc: 0.725000\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss: 0.730044 Val_Acc: 0.900000\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss: 0.894383 Val_Acc: 0.725000\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss: 0.869701 Val_Acc: 0.800000\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss: 0.981095 Val_Acc: 0.700000\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss: 0.778734 Val_Acc: 0.800000\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss: 0.701533 Val_Acc: 0.875000\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss: 0.863021 Val_Acc: 0.775000\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss: 0.839261 Val_Acc: 0.825000\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss: 0.998160 Val_Acc: 0.725000\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss: 0.785209 Val_Acc: 0.775000\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss: 0.682042 Val_Acc: 0.850000\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss: 0.854183 Val_Acc: 0.725000\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss: 0.814163 Val_Acc: 0.800000\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss: 0.940133 Val_Acc: 0.725000\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss: 0.747550 Val_Acc: 0.825000\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss: 0.634736 Val_Acc: 0.900000\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss: 0.847869 Val_Acc: 0.725000\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss: 0.815435 Val_Acc: 0.800000\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss: 0.933238 Val_Acc: 0.725000\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss: 0.731383 Val_Acc: 0.825000\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss: 0.621874 Val_Acc: 0.900000\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss: 0.813041 Val_Acc: 0.700000\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss: 0.795859 Val_Acc: 0.750000\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss: 0.919651 Val_Acc: 0.750000\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss: 0.722250 Val_Acc: 0.825000\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss: 0.625200 Val_Acc: 0.950000\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss: 0.774064 Val_Acc: 0.825000\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss: 0.783495 Val_Acc: 0.850000\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss: 0.919065 Val_Acc: 0.725000\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss: 0.669066 Val_Acc: 0.850000\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss: 0.587920 Val_Acc: 0.925000\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss: 0.789160 Val_Acc: 0.750000\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss: 0.750963 Val_Acc: 0.825000\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss: 0.915163 Val_Acc: 0.725000\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss: 0.681724 Val_Acc: 0.800000\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss: 0.570656 Val_Acc: 0.925000\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss: 0.735121 Val_Acc: 0.800000\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss: 0.738134 Val_Acc: 0.850000\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss: 0.881051 Val_Acc: 0.725000\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss: 0.654406 Val_Acc: 0.850000\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss: 0.550168 Val_Acc: 0.925000\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss: 0.722576 Val_Acc: 0.850000\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss: 0.725524 Val_Acc: 0.825000\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss: 0.873322 Val_Acc: 0.725000\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss: 0.654955 Val_Acc: 0.825000\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss: 0.553033 Val_Acc: 0.925000\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss: 0.692418 Val_Acc: 0.850000\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss: 0.719386 Val_Acc: 0.825000\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss: 0.884941 Val_Acc: 0.725000\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss: 0.626885 Val_Acc: 0.850000\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss: 0.548273 Val_Acc: 0.925000\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss: 0.681295 Val_Acc: 0.800000\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss: 0.677627 Val_Acc: 0.850000\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss: 0.878882 Val_Acc: 0.700000\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss: 0.617474 Val_Acc: 0.825000\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss: 0.537305 Val_Acc: 0.925000\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss: 0.650991 Val_Acc: 0.825000\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss: 0.681517 Val_Acc: 0.875000\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss: 0.842351 Val_Acc: 0.750000\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss: 0.604068 Val_Acc: 0.850000\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss: 0.529898 Val_Acc: 0.925000\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss: 0.645724 Val_Acc: 0.850000\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss: 0.647906 Val_Acc: 0.850000\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss: 0.823676 Val_Acc: 0.750000\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss: 0.595672 Val_Acc: 0.850000\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss: 0.497799 Val_Acc: 0.950000\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss: 0.624818 Val_Acc: 0.825000\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss: 0.639631 Val_Acc: 0.850000\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss: 0.825376 Val_Acc: 0.725000\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss: 0.565395 Val_Acc: 0.825000\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss: 0.495289 Val_Acc: 0.950000\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss: 0.596080 Val_Acc: 0.875000\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss: 0.623616 Val_Acc: 0.850000\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss: 0.799622 Val_Acc: 0.725000\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss: 0.568488 Val_Acc: 0.850000\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss: 0.472150 Val_Acc: 0.950000\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss: 0.583137 Val_Acc: 0.875000\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss: 0.607764 Val_Acc: 0.875000\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss: 0.777498 Val_Acc: 0.700000\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss: 0.535914 Val_Acc: 0.850000\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss: 0.456951 Val_Acc: 0.950000\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss: 0.581072 Val_Acc: 0.850000\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss: 0.569396 Val_Acc: 0.900000\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss: 0.795133 Val_Acc: 0.725000\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss: 0.526203 Val_Acc: 0.825000\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss: 0.468628 Val_Acc: 0.925000\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss: 0.567044 Val_Acc: 0.850000\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss: 0.599820 Val_Acc: 0.875000\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss: 0.770190 Val_Acc: 0.725000\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss: 0.515580 Val_Acc: 0.825000\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss: 0.441421 Val_Acc: 0.950000\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss: 0.536821 Val_Acc: 0.900000\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss: 0.564290 Val_Acc: 0.875000\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss: 0.754201 Val_Acc: 0.725000\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss: 0.518413 Val_Acc: 0.825000\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss: 0.428808 Val_Acc: 0.925000\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss: 0.529875 Val_Acc: 0.875000\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss: 0.583947 Val_Acc: 0.825000\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss: 0.724241 Val_Acc: 0.750000\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss: 0.489771 Val_Acc: 0.875000\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss: 0.407122 Val_Acc: 0.950000\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss: 0.516672 Val_Acc: 0.900000\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss: 0.540457 Val_Acc: 0.875000\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss: 0.734304 Val_Acc: 0.750000\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss: 0.483784 Val_Acc: 0.875000\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss: 0.416292 Val_Acc: 0.925000\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss: 0.517895 Val_Acc: 0.850000\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss: 0.519826 Val_Acc: 0.875000\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss: 0.712953 Val_Acc: 0.775000\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss: 0.458871 Val_Acc: 0.900000\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss: 0.385039 Val_Acc: 0.950000\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss: 0.474694 Val_Acc: 0.950000\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss: 0.512374 Val_Acc: 0.875000\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss: 0.721729 Val_Acc: 0.775000\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss: 0.440184 Val_Acc: 0.925000\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss: 0.396048 Val_Acc: 0.950000\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss: 0.478910 Val_Acc: 0.925000\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss: 0.502186 Val_Acc: 0.875000\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss: 0.696761 Val_Acc: 0.775000\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss: 0.427919 Val_Acc: 0.875000\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss: 0.393310 Val_Acc: 0.950000\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss: 0.466348 Val_Acc: 0.950000\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss: 0.495656 Val_Acc: 0.875000\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss: 0.660909 Val_Acc: 0.750000\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss: 0.419675 Val_Acc: 0.925000\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss: 0.378411 Val_Acc: 0.925000\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss: 0.460871 Val_Acc: 0.925000\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss: 0.493738 Val_Acc: 0.875000\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss: 0.671784 Val_Acc: 0.775000\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss: 0.407317 Val_Acc: 0.900000\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss: 0.340208 Val_Acc: 0.975000\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss: 0.417777 Val_Acc: 0.925000\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss: 0.483904 Val_Acc: 0.875000\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss: 0.650706 Val_Acc: 0.775000\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss: 0.413111 Val_Acc: 0.875000\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss: 0.357346 Val_Acc: 0.975000\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss: 0.411403 Val_Acc: 0.925000\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss: 0.480936 Val_Acc: 0.850000\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss: 0.654769 Val_Acc: 0.775000\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss: 0.400972 Val_Acc: 0.925000\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss: 0.348789 Val_Acc: 0.975000\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss: 0.417355 Val_Acc: 0.925000\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss: 0.443278 Val_Acc: 0.900000\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss: 0.617249 Val_Acc: 0.775000\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss: 0.388762 Val_Acc: 0.925000\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss: 0.332753 Val_Acc: 0.950000\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss: 0.420764 Val_Acc: 0.925000\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss: 0.429433 Val_Acc: 0.850000\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss: 0.596129 Val_Acc: 0.800000\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss: 0.368766 Val_Acc: 0.925000\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss: 0.323939 Val_Acc: 0.975000\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss: 0.403080 Val_Acc: 0.950000\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss: 0.433020 Val_Acc: 0.900000\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss: 0.626765 Val_Acc: 0.800000\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss: 0.375177 Val_Acc: 0.950000\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss: 0.329375 Val_Acc: 0.950000\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss: 0.386075 Val_Acc: 0.975000\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss: 0.422448 Val_Acc: 0.900000\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss: 0.596958 Val_Acc: 0.825000\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss: 0.352211 Val_Acc: 0.925000\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss: 0.309686 Val_Acc: 0.925000\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss: 0.377115 Val_Acc: 0.950000\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss: 0.411200 Val_Acc: 0.900000\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss: 0.554851 Val_Acc: 0.825000\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss: 0.341019 Val_Acc: 0.950000\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss: 0.312084 Val_Acc: 0.975000\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss: 0.373759 Val_Acc: 0.950000\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss: 0.393045 Val_Acc: 0.900000\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss: 0.559709 Val_Acc: 0.825000\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss: 0.334873 Val_Acc: 0.950000\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss: 0.305469 Val_Acc: 0.975000\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss: 0.379460 Val_Acc: 0.950000\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss: 0.379259 Val_Acc: 0.900000\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss: 0.562648 Val_Acc: 0.800000\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss: 0.330023 Val_Acc: 0.950000\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss: 0.287009 Val_Acc: 0.975000\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss: 0.349139 Val_Acc: 0.950000\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss: 0.384056 Val_Acc: 0.900000\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss: 0.547067 Val_Acc: 0.800000\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss: 0.319911 Val_Acc: 0.950000\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss: 0.293027 Val_Acc: 0.975000\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss: 0.348066 Val_Acc: 0.950000\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss: 0.381951 Val_Acc: 0.900000\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss: 0.536523 Val_Acc: 0.875000\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss: 0.316120 Val_Acc: 0.950000\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss: 0.278591 Val_Acc: 0.950000\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss: 0.334456 Val_Acc: 0.975000\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss: 0.351170 Val_Acc: 0.925000\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss: 0.534257 Val_Acc: 0.875000\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss: 0.300905 Val_Acc: 0.975000\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss: 0.280227 Val_Acc: 0.950000\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss: 0.330332 Val_Acc: 0.975000\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss: 0.346496 Val_Acc: 0.925000\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss: 0.496788 Val_Acc: 0.875000\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss: 0.288790 Val_Acc: 0.950000\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss: 0.266439 Val_Acc: 0.975000\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss: 0.304005 Val_Acc: 0.975000\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss: 0.332177 Val_Acc: 0.925000\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss: 0.509097 Val_Acc: 0.875000\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss: 0.293434 Val_Acc: 0.975000\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss: 0.270041 Val_Acc: 0.950000\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss: 0.312636 Val_Acc: 0.950000\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss: 0.347761 Val_Acc: 0.900000\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss: 0.473252 Val_Acc: 0.875000\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss: 0.297048 Val_Acc: 0.950000\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss: 0.279343 Val_Acc: 0.975000\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss: 0.324775 Val_Acc: 0.975000\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss: 0.331772 Val_Acc: 0.900000\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss: 0.477817 Val_Acc: 0.850000\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss: 0.258664 Val_Acc: 0.975000\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss: 0.271586 Val_Acc: 0.975000\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss: 0.307016 Val_Acc: 0.950000\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss: 0.337610 Val_Acc: 0.900000\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss: 0.493710 Val_Acc: 0.825000\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss: 0.278866 Val_Acc: 0.975000\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss: 0.252981 Val_Acc: 0.975000\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss: 0.298604 Val_Acc: 0.950000\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss: 0.325555 Val_Acc: 0.925000\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss: 0.454760 Val_Acc: 0.850000\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss: 0.259820 Val_Acc: 0.950000\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss: 0.251961 Val_Acc: 0.950000\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss: 0.293421 Val_Acc: 0.975000\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss: 0.309838 Val_Acc: 0.925000\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss: 0.463176 Val_Acc: 0.875000\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss: 0.242957 Val_Acc: 0.975000\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss: 0.232579 Val_Acc: 0.975000\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss: 0.294727 Val_Acc: 0.950000\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss: 0.306371 Val_Acc: 0.925000\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss: 0.452381 Val_Acc: 0.875000\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss: 0.230579 Val_Acc: 0.975000\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss: 0.239544 Val_Acc: 0.975000\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss: 0.283605 Val_Acc: 0.950000\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss: 0.278343 Val_Acc: 0.925000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.63115234375\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZc6uXC85cgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bf074814a8>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-70% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 70%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
